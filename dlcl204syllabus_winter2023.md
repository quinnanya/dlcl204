# Digital Humanities Across Borders

_DLCL 204, CompLit 204A_

_Winter 2023_

Quinn Dombrowski (DLCL Academic Technology Specialist)

For examples of past student projects in this class, visit the [DLCL 204 page on the Stanford DH website](https://digitalhumanities.stanford.edu/dlcl204/)

# Overview

English-language resources have dominated digital humanities across the globe. This course takes a broader view, focusing on the methods, tools, and discourse of digital humanities as applied to textual materials in languages other than English. Students will develop practical skills in applying digital research methods to texts in any language of their choosing. In addition, students will become familiar with major digital humanities scholarly organizations, movements, and debates that have their origins in different linguistic and cultural identities. No prior technical or digital humanities experience required, but students must have a reading knowledge of at least one non-English language (modern or historical).

# Credits / Grading

This course will be offered for 3-5 credits, and for either a grade or credit/no credit. Students who wish to take it as part of the DH Minor must choose 5 credits (if taken as a core course) or 3 credits (if taken as an elective) and must take it for a letter grade.

The course will use contract grading, where students choose what grade they wish to receive, and write a contract (within defined parameters) at the beginning of the quarter that lays out the requirements for receiving that grade. Individual assignments will receive extensive feedback but will be graded as accepted / needs revisions. Students will have one week to revise assignments that need revisions to fulfill the terms of their contract. If a student is unable to fulfill the terms of their original contract, they will meet with the instructor and sign a new contract for a different grade.

# Assignments

This course will involve doing less reading than most courses in the humanities, and there will be neither papers nor exams. Instead, you'll get experience with reading and producing different kinds of scholarly production through 3-4 "major assignments" that will take the form of some alternative mode of scholarly creation related to your own project (e.g. data sets, conference proposals, posters, tutorials, blog posts, podcasts, videos, games, creative physical or digital visualizations).

Most class sessions will have materials for you to read or review in advance, and discuss as part of class. The selection of texts will be shaped by students' languages and interests, so this preliminary list will likely be updated over the course of the quarter. Your final assignment will bring together the work that you've done with your language as well as the some of the readings and ideas from class discussion, using a note-taking and web publishing tool called [Obsidian](https://obsidian.md/).

Likewise, most class sessions will have time for you to try out different tools or methods, and/or work on your next major assignment. Because students will be using different tools depending on their interests and language(s), there will be a mix of live demonstrations, and working through written or video tutorials independently or in a group with other students using similar methods or working in the same language -- with assistance if you get stuck. Plan on bringing a laptop to class regularly!

## Major assignments

Your path through this course will be shaped by the language and text you're working with, and your research questions / area of inquiry. Almost all DH projects that involve share the same broad trajectory:

* **Acquisition**: Getting your material in a form that you can use for the subsequent steps. Can include scanning, OCR (turning pictures of text into editable text), handwritten text recognition, web scraping, downloading and transcribing video, etc.
* **Enrichment**: Doing something to your materials to make it possible to do analysis / visualization. May include various forms of cleaning the data, annotating it (manually, or using algorithms to extract person and place names), adding metadata, etc.
* **Analysis/visualization**: Analyzing or visualizing the text in some way using the enrichment you've done. Includes things like topic modeling, text comparison, network analysis, mapping, etc.
* **Interpretation**: What sense can you actually make of the analysis, given what you know already about the subject matter? 

The course will be broken up into segments based on these different phases. Students will complete one major assignment for text acquisition, enrichment, and analysis/visualization (3 credits). 5-credit students will also complete a major assignment for interpretation. 

Major assignments can take many different formats, for instance:

* A set of texts that you've scanned and OCR'd, with a 1-page written reflection on the process (what worked well, what was challenging, what surprised you, etc.)
* A collaborative podcast with some of the other students working on the same language or tool, comparing your experiences working on a particular phase of the project
* A step-by-step tutorial for how to use one of the tools for your project, including details specific to your language
* A pair of conference proposals for your project, one for a conference in your discipline, and another for a DH conference
* A choose-your-own-adventure game/story that takes the reader through major decision points when working on a textual DH project in your language
* A *data physicalization* (a visualization with a physical form, e.g. 3D printing, knitting or sewing) of your data that captures some significant trait in the data, along with a one-page narrative.

Other creative formats are also welcome! Check with me if you have any questions.

By default, each major assignment will be due after the corresponding phase of the class, to help you stay on track throughout the quarter. That said, the deadlines can be flexible if you need more time on one or more of the assignments; please let me know if you'd like an extension. All work should be turned in by March 17th, so I can get you feedback and you can do any revisions prior to the *absolutely final deadline* of March 22 (grades are due on the 28th).


# Weekly schedule

## Week 1 (January 9th & 11th): Introductions
One of the challenges of teaching this course is that I don't know what languages people are interested in until they arrive. (If you're enrolling in the class, it helps if you email me in advance and let me know what language you'll be working with!) The first week will give us a chance to get to know one another, and I'll try to find some existing DH work that might resonate with your linguistic and topical interests. I'll also share a few different perspectives on what DH is, and what it looks like in practice to do DH work.

During week 1, I'll set up times for separate 1:1 meetings with students to talk about what they're interested in working on this quarter.

This week's readings are an introduction to major themes you'll encounter in the communities that make up digital humanities:

* Lisa Spiro's "['This is Why We Fight: Defining the Values of Digital Humanities](https://dhdebates.gc.cuny.edu/read/untitled-88c11800-9446-469b-a3be-3fdb36bfbd1e/section/9e014167-c688-43ab-8b12-0f6746095335)" (2012) looks to define shared values as a way to find common ground within the (still ongoing) debates about what DH is.
* The piece "[Donner à lire les humanités numériques francophones](https://journals.openedition.org/revuehn/508)" (2020) from the editorial board of *Humanités Numériques* (the journal of the Francophone DH association [Humanistica](http://www.humanisti.ca/)) talks about how the journal came to be, as part of the process of forming and defining the field of Francophone DH. If you don't comfortably read French, Google Translate works fine, but be sure to check back with the original particularly around terminology, which gets a little lost in the translation (e.g. in paragraph 5, '*"Digital Humanities" materialized in English-speaking countries - then everywhere else, sometimes with local names. The genesis of the journal is, in fact, deeply rooted in the European and French development of digital humanities.'* loses the fact that the first reference to "digital humanities" uses the English term, and the second uses the French term.)
* "[Digital Humanities: Is it Research or is it Service?](https://dhmuc.hypotheses.org/2834)" by Arnold Eckhart (2020) on the Digital Humanities München blog gets more concrete about what kinds of tasks go into doing DH, and reflects on the extent to which they should be considered "research" or "service". (Over the course of the quarter, you'll see other perspectives that challenge how meaningful or useful this dichotomy is, but it covers some common ways of talking and thinking about DH.)


## Week 2 (January 18): Text acquisition

**Note: Monday, January 16th is a holiday**

Students will start to work on the text acquisition phase of their project.

* [Data-Sitters Club #7: The DSC and Mean Copyright Law](https://datasittersclub.github.io/site/dsc7.html) and [Data-Sitters Club #14: Hello, DMCA Exemption](https://datasittersclub.github.io/site/dsc14.html) cover the legal landscape and recent developments around how you can use encrypted ebooks for research.
* [‘Q i-jtb the Raven’: Taking Dirty OCR Seriously](https://ryancordell.org/research/qijtb-the-raven/), Ryan Cordell, 2017. This piece dives into the metadata you leave behind in the EXIF file structure, as someone who digitizes a text that was originally printed. How are digitized works more than just a machine-readable version of the original? What's the "materiality" of a digitized text? As you read this, think about your own digitized texts. What path do you think they took to get that way? If you're scanning and OCRing books, think about how your choices impact the final version. The references to "CA" are about "Chronicling America" an effort to digitize American newspapers that excluded Japanese-language newspapers, among others. (Locally, the [Hoover Institute](https://www.hoover.org/library-archives/collections/japan) has a project to fill in that gap.)
* [Recognizing handwritten text in Slavic manuscripts: A neural-network approach using Transkribus](https://www.academia.edu/38835297/Recognizing_handwritten_text_in_Slavic_manuscripts_A_neural_network_approach_using_Transkribus_1_Achim_Rabus) by Achim Rabus. This piece is, fundamentally, a write-up of one scholar's experiences with using the Transkribus software to create a model for digitizing Slavic manuscripts. The Russian folks in the class have a leg up here, but honestly, even if you can't recognize any of the Cyrillic alphabet, reading this is a very real-world experience. (I don't know the first thing about Ottoman Turkish, but I'm reading some comp sci papers about machine learning methods on how to translate it, this week.) Don't get bogged down into the techical (or Slavistic) details if that's not your thing. What are the use cases where Achim describes trying it out? Does Transkribus work well for all those use cases? If you were teaching a DH class and you had a student interested in Slavic Cyrillic manuscripts, would you recommend it? Why or why not?
* Watch [this news report on Tarin Clanuwat and her machine-learning model for kuzushiji](https://www.facebook.com/cnbctv18india/posts/2540954169275299). This is for a general audience; you can also read more detail on the [Around DH 2020 piece](http://arounddh.org/en/kuronet) about the project.

## Week 3 (January 23 & January 25th): Unicode / preparing for enrichment

We'll talk about Unicode, what it is, and why it's important. We'll also take a field trip to Special Collections and explore the Unicode Archive, with an eye towards choosing some items for an upcoming exhibit.

Students will continue to work on text acquisition during week 3, and will prepare their assignment for the text acquisition phase. I'll be available throughout the week for troubleshooting help and advice.

* With a week and a half until the first major assignment is due, you should be working on preparing your folder of plain-text files and your spreadsheet with information about each of the text files. Some of you will only have a couple texts, so it won't be hard to make sure that the spreadsheet is consistent. Some of you are going to have more than 20 texts, and the more texts you have, the easier it is to make mistakes. Read through [this tutorial on OpenRefine](https://programminghistorian.org/en/lessons/cleaning-data-with-openrefine) by Seth van Hooland, Ruben Verborgh, and Max De Wilde, and try it out on your spreadsheet if you think it'd be helpful. (You don't need to annotate this one, but feel free to ask me any questions.)
* Following up on last week's lecture on Unicode, we have a critique of Unicode from the perspective of its imposition of a western, alphabetic model of semiotics on scripts whose own organizing principles take a different form (using Devanagari and Arabic script as examples). Paolo Monella was kind enough to share a preprint of "Scritture dimenticate, scritture colonizzate: sistemi grafici e codifiche digitali nelle culture araba e indiana" with us, and I've put in the Canvas module both the preprint PDF itself (which renders all the text correctly), and a version where I exported the text from the PDF, much like many of you are doing for your corpora. In a moment of irony that really drives home some of the points this piece is making, exporting the text from Acrobat resulted in all the non-Latin characters being dropped altogether, and many of the Latin characters with diacritics were also dropped. So the references to "fu" are in fact *fuṣḥа̄*. The piece is in Italian, and at least when I ran the whole page through Google Translate in the browser, some arbitrary paragraphs didn't translate. But if you put those into Google Translate individually, it works fine.
* In anticipation of next week when we'll start working on enrichment, "[Preparing Non-English Texts for Computational Analysis](https://www.modernlanguagesopen.org/articles/10.3828/mlo.v0i0.294/)" (which I wrote last year) covers some of the major things that you need to do to non-English text before using it with any kind of analysis involving counting words.


## Week 4 (January 30 & February 1): Enrichment

This week you should be close to having your first assignment (your corpus) ready. (If not, let me know and we'll figure something out!) Now that you have some text, it's time to start doing things with it. You'll soon be splitting off again to work on your respective projects, but for one week, let's start exploring your many (and very different) corpora using the same tool: [Voyant](https://voyant-tools.org/). This will also be the week when you'll get to try out Jupyter notebooks for pre-processing your text -- and you can compare how raw vs. pre-processed non-English text behaves in Voyant. Here's the combo reading list / to-do list for this week:

* Ch. 1 ("Introduction: Correcting Method") and Ch. 2 ("The Measured Words: How Computers Analyze Texts") of *Hermeneutica: Computer-Assisted Interpretation in the Humanities* by Stéfan Sinclair & Geoffrey Rockwell. These chapters form an introduction to thinking about what humanities-oriented computational text analysis is, and what it does (and what it can't do). Hermeneutica is [available online via Searchworks](https://searchworks.stanford.edu/view/13347297), and I'll have Hypothesis-enabled versions on Canvas.
* **Do**: Upload the text files from your corpus to [Voyant](https://voyant-tools.org/) and explore some of the different tools available. What do you notice? How well does it work?
* **Read/Do**: [Introduction to Jupyter Notebooks](https://programminghistorian.org/en/lessons/jupyter-notebooks): this Programming Historian tutorial explains what Jupyter notebooks are, both conceptually and practically. Follow the instructions in the tutorial to install Anaconda on your computer. Many of you will be using Jupyter notebooks for text pre-processing.
* **Do**: For languages other than Chinese, Japanese, and Vietnamese, download the lemmatizer notebook for your language, and run it on your text to create lemmatized derivative files (where all words are replaced with their dictionary forms). Upload **those** files to Voyant. How are your results different? (Chinese, Japanese, and Vietnamese-language students can skip this step.)
* **Read**: For a colloquial walkthrough of how you might follow a research question through multipel Voyant tools, here's [Data-Sitters Club #6: Voyant's Big Day](https://datasittersclub.github.io/site/dsc6/).


## February 3: Major assignment 1 due
The text acquisition major assignment is due, unless you've arranged for a different due date.

## Week 5 (February 6 & 8): Word vectors & Principal Component Analysis
Word vectors aren't the place that you'd usually start with exploring your text -- or even the place you'd go next after Voyant. But if more than a couple of you are interested in playing with word vectors as part of your final project, and in most cases you want to use the lemmatized version of your corpus for word vectors -- and last week we covered how to lemmatize your corpus. Next week we'll get into text comparison, which will require you to do part-of-speech tagging on your corpus, but for this week, you can use the lemmatized versions of your texts for word vectors.

For interested students, we can also explore Principal Component Analysis as a way to cluster "similar" (for a certain value of similar) texts together.

* **Read**: Read Ryan Heuser's his 4-part series about word vectors: [part 1](https://ryanheuser.org/word-vectors-1/), [part 2](https://ryanheuser.org/word-vectors-2/), [part 3](https://ryanheuser.org/word-vectors-3/), [part 4](https://ryanheuser.org/word-vectors-4/)
* **Try** generating word vectors based on your corpus using this Jupyter notebook. If your corpus has less than 1 million words, you might want to try playing with one of the [pre-trained models for your language](https://fasttext.cc/docs/en/pretrained-vectors.html) (e.g. these models are provided by Facebook).
* For the Classics students, here's a paper on "[Word Embeddings Pointing the Way for Late Antiquity](https://www.aclweb.org/anthology/W15-3708.pdf)" by Johannes Bjerva and Raf Praet (no need to annotate, just FYI)
* **Read**: [Data-Sitters Club #10: Heather Likes Principal Component Analysis](https://datasittersclub.github.io/site/dsc10.html)


## Week 6 (February 13 & 15): Catch-up & AI text generation
This week is a good opportunity to catch up on any material you haven't had a chance to get through, and start working on a plan for your final projet, which should help shape decisions about what you want to do for assignments 2 and 3.

Depending on students interests, we may explore different language-specific tools for AI text generation, to better understand how they work and how we might use them.

* **Read**: [Data-Sitters Club #9: The Ghost in Anouk's Laptop](https://datasittersclub.github.io/site/dsc9.html)


## February 17: Major assignment 2 due
The enrichment major assignment is due, unless you've arranged for a different due date.

## Week 7 (February 22): Named entity recognition

**Note: Monday, February 20 is a holiday**

A number of students might be interested in the people and places that occur in their corpus. Named entity recognition is one way to try to identify those, particularly when you don't have a pre-defined list of names or places that you're interested in. (If you do, you're better off just searching for the things on your list, in a manner sensitive to possible variations.)

* Read: *[Data-Sitters Club Multilingual Mystery #2: Beware Lee and Quinn!](https://datasittersclub.github.io/site/dscm2/): this DSC book compares how well the Spacy named entity recognition for English works, compared to French. (You can guess which one works better.) There's a link to a Jupyter notebook with code for running Spacy NER for English and French; if your language's lemmatizer code used Spacy, you can try adapting the code in the DSC notebook for your language. (All you should have to do is change which model it loads; compare the first part of the notebook to your language's lemmatizer notebook.) If your language didn't use Spacy, talk with Quinn about what other tools you can use. (If you'd like, Quinn can put together a notebook that you can try out for your language.)
* Read: Katherine McDonough, Ludovic Moncla & Matje van de Camp. "[Named entity recognition goes to old regime France: geographic text analysis for early modern French corpora](https://www.tandfonline.com/doi/abs/10.1080/13658816.2019.1620235)". 


## Week 8 (February 27) & Week 9 (March 6): Project work / topic modeling
Over these two weeks, students will mostly be working on their own projects, which will lead to major assignment 3 (and 4, for 5-credit students). Based on students' project needs, I'll be putting together tutorials around specific topics, including potentially training models, network visualizations, and general data visualization with Tableau. The last method that we'll cover together is topic modeling.

* Read: *[The Joy of Topic Modeling](http://mcburton.net/blog/joy-of-tm/)* by Matt Burton
* Read: *[The LDA Buffet is Now Open; or, Latent Dirichlet Allocation for English Majors](https://www.matthewjockers.net/2011/09/29/the-lda-buffet-is-now-open-or-latent-dirichlet-allocation-for-english-majors/)* by Matt Jockers
* Play with [jsLDA: In-browser topic modeling](https://mimno.infosci.cornell.edu/jsLDA/) (remember, if you're using your own data, you need to upload multiple "documents", AKA "some piece of text"). You may also enjoy [this tweet stream](https://twitter.com/dmimno/status/1320736660150198273?s=19) by David Mimno explaining the design goals.

## Week 10 (March 13 & 15): Wrap-up
As a class we'll consider what we've learned, and how we can apply it moving forward in different contexts (in other classes, as CV/resumé fodder, as a way to pursue questions that interest you. 

## March 17: Major assignment 3 due
The analysis major assignment is due, unless you've arranged for a different due date.

## March 24: All work due
March 24th is the deadline for turning in all work (including major assignment 4 for 5-credit students), with enough time for me to send you feedback and you to do revisions, if needed.

## Slack
Everyone will be working on their projects at a different pace, at different times of day. To avoid the turnaround time of exchanging email every time you have a question, I'll set up a Slack channel for the class, or you can message me privately. Use of Slack is not required, but may make it faster to get the answers you're looking for -- from me, or your classmates.

# Accommodation

I want this class to be accessible to anyone who shows up with an interest in the topic. Everyone has a right to the full experience of the class — which is fundamentally about what role digital tools and methods might play in your education and/or research, and how you might choose to connect with and contribute to communities of "digital" scholarship. Grappling with your identity as a student and/or scholar can be difficult. If there's something in the syllabus or that we cover in class that freaks you out, please talk to me! If you're encountering a barrier, or expecting a possible barrier to your being able to successfully complete your contract, let's chat! It doesn't need to be academic — sleep-training kids, taking care of unwell family and friends, roommates who listen to Viking metal bands at full volume at all hours of the night, and anxiously binge-watching Netflix are real life. Mental illness flare-ups can impact what you can get done — I know, I had a bipolar II diagnosis for all of grad school. Whether or not you've got paperwork through the Office of Accessible Education (<https://oae.stanford.edu/>), please come talk to me anytime if something is getting in the way of your learning or completing the contract you created.

The pandemic and all ongoing uncertainty around it are complicating life for everyone, in different ways. Regardless of your situation, I want this class to be an enriching, thought-provoking experience for you. Please get in touch (using any contact methods listed on Canvas that's comfortable for you) if anything comes up that you think might pose a challenge for you to successfully complete the class, and together we can figure out a way to work things out.

# Fundamental standard & honor code

Beyond the bare minimums laid out by the Stanford Fundamental Standard (<https://communitystandards.stanford.edu/policies-and-guidance/fundamental-standard>), I expect you to treat one another in this class not only with respect, but with generosity. If you find a resource or an approach that has helped you, share it so others can benefit -- and listen when others share.

The honor code (<https://communitystandards.stanford.edu/policies-and-guidance/honor-code>) lays out guidelines for the university's policy on academic integrity. Collaboration -- with your classmates and others -- is very much welcome in this course, but be sure to acknowledge your collaborators and the assistance they provided (e.g. including in your acknowledgements section a cousin who helped you interpret some of the statistics from your analysis.)


# Acknowledgements

Thanks to Miriam Posner (DH 150: Selfies, Snapchat, & Cyberbullies, <http://miriamposner.com/dh150w15/contract-grading/>) and Ryan Cordell (ENGL 1450: Reading and Writing in the Digital Age, <https://f18rwda.ryancordell.org/course-policies.html>) for the inspiration for contract grading.

Thanks to Aimée Morrison for inspiring the original accommodation statement.
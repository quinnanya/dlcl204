{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vectors\n",
    "\n",
    "This Jupyter notebook walks you through the steps of creating word vectors from your (lemmatized, if needed) source texts, and runs you through doing various kinds of analysis on those word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting the source texts\n",
    "This notebook assumes you've already acquired texts that you want to work with. You need, at a minimum, around 1 million words to get better-than-garbage results for word vectors, and your texts need to be lemmatized (if you're working with a language where that's relevant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the source texts\n",
    "The lemmatization code for your language may or may not already take care of some of these cleaning steps -- and also, these cleaning steps might not be all you need. If you have other punctuation that's causing a problem, try modifying the examples below, and/or you can always check in with Quinn about it.\n",
    "\n",
    "Even if you don't need to clean your source texts, be sure to run the first code block to import modules that will be important for the word vector steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing modules and setting up paths\n",
    "Change the value of *sourcefiledirectory* to where you've put your source files, then run the code block below first, even if you want to move on immediately to the word vectors. It imports a number of modules you'll need later.\n",
    "\n",
    "For instance, the default path a text file in the Documents directory is (substituting your user name on the computer for YOUR-USER-NAME):\n",
    "\n",
    "* On Mac: '/Users/YOUR-USER-NAME/Documents/YOUR-TEXT-FILE.txt'\n",
    "* On Windows: 'C:\\\\\\Users\\\\\\YOUR-USER-NAME\\\\\\Documents\\\\\\YOUR-TEXT-FILE.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os is used for things like changing directories and listing files\n",
    "import os\n",
    "#io is used for opening and writing files\n",
    "import io\n",
    "#itertools is used for some of the iterative code\n",
    "from itertools import chain\n",
    "#glob is used to find all the pathnames matching a specified pattern (here, all text files)\n",
    "import glob\n",
    "\n",
    "#This is the full path to the directory where you've stored the source texts\n",
    "sourcefiledirectory = '/Users/qad/Documents/hp_noparatext'\n",
    "\n",
    "#Changing the directory to where you've stored the source texts, so you can open them in later code blocks\n",
    "os.chdir(sourcefiledirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Lower-casing all text\n",
    "The code below **replaces** your source files with versions where all characters are lower-case. Be sure you have a copy of the original version of your source file somewhere else in case you need to go back to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look through the directory you specified to find files that end in .txt.\n",
    "for filename in os.listdir(sourcefiledirectory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        #For each file that ends in .txt, open and read its contents into a string. Then make the characters lower-case.\n",
    "        f = open(filename, 'r', encoding='utf8')\n",
    "        text = f.read()\n",
    "        lines = [text.lower()]\n",
    "\n",
    "        #Create a new file with the same file name (i.e. replacing the original file) and write the lowercase lines\n",
    "        #This method also automatically closes the file once it's done\n",
    "        with open(filename, 'w', encoding='utf8') as out:\n",
    "            out.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Removing line breaks\n",
    "Line breaks get attached to the previous word, so this code adds a space to separate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for files in the source directory that end in .txt\n",
    "for filename in os.listdir(sourcefiledirectory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        #Read each text file into a string. Find newline characters (\\n) and put a space before and after.\n",
    "        f = open(filename, 'r', encoding='utf8')\n",
    "        text = f.read()\n",
    "        lines = [text.replace(\"\\n\", \" \\n \")]\n",
    "\n",
    "        #Write output to a new file with the same name as the original, overwriting the original file.\n",
    "        with open(filename, 'w', encoding='utf8') as out:\n",
    "            out.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Cleaning up punctuation\n",
    "You need to either remove punctuation attached to words, or separate it from the words with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gets rid of ellipses. Sets of more than one period complicate further text processing.\n",
    "\n",
    "for filename in os.listdir(sourcefiledirectory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        #Read each text file into a string, and do the find-and-replace\n",
    "        f = open(filename, 'r', encoding='utf8')\n",
    "        text = f.read()\n",
    "        lines = [text.replace(\"...\", \"\")]\n",
    "        \n",
    "        #Write output to a new file with the same name as the original, overwriting the original file.\n",
    "        with open(filename, 'w', encoding='utf8') as out:\n",
    "            out.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gets rid of sets of two periods (yes, there were some of those in the source files!)\n",
    "\n",
    "for filename in os.listdir(sourcefiledirectory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        #Read each text file into a string, and do the find-and-replace\n",
    "        f = open(filename, 'r', encoding='utf8')\n",
    "        text = f.read()\n",
    "        lines = [text.replace(\"..\", \"\")]\n",
    "        \n",
    "        #Write output to a new file with the same name as the original, overwriting the original file.\n",
    "        with open(filename, 'w', encoding='utf8') as out:\n",
    "            out.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a period followed by a space, and puts a space before it as well.\n",
    "# You don't want to just get rid of all periods because they're used in abbreviations.\n",
    "\n",
    "for filename in os.listdir(sourcefiledirectory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        #Read each text file into a string, and do the find-and-replace\n",
    "        f = open(filename, 'r', encoding='utf8')\n",
    "        text = f.read()\n",
    "        lines = [text.replace(\". \", \" . \")]\n",
    "        \n",
    "        #Write output to a new file with the same name as the original, overwriting the original file.\n",
    "        with open(filename, 'w', encoding='utf8') as out:\n",
    "            out.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes the period is followed by a newline rather than a space, so this code puts a space before those.\n",
    "\n",
    "for filename in os.listdir(sourcefiledirectory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        #Read each text file into a string, and do the find-and-replace\n",
    "        f = open(filename, 'r', encoding='utf8')\n",
    "        text = f.read()\n",
    "        lines = [text.replace(\".\\n\", \" .\\n\")]\n",
    "        \n",
    "        #Write output to a new file with the same name as the original, overwriting the original file.\n",
    "        with open(filename, 'w', encoding='utf8') as out:\n",
    "            out.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets rid of colons\n",
    "\n",
    "for filename in os.listdir(sourcefiledirectory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        #Read each text file into a string, and do the find-and-replace\n",
    "        f = open(filename, 'r', encoding='utf8')\n",
    "        text = f.read()\n",
    "        lines = [text.replace(\":\", \"\")]\n",
    "        \n",
    "        #Write output to a new file with the same name as the original, overwriting the original file.\n",
    "        with open(filename, 'w', encoding='utf8') as out:\n",
    "            out.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets rid of semicolons\n",
    "\n",
    "for filename in os.listdir(sourcefiledirectory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        #Read each text file into a string, and do the find-and-replace\n",
    "        f = open(filename, 'r', encoding='utf8')\n",
    "        text = f.read()\n",
    "        lines = [text.replace(\";\", \"\")]\n",
    "        \n",
    "        #Write output to a new file with the same name as the original, overwriting the original file.\n",
    "        with open(filename, 'w', encoding='utf8') as out:\n",
    "            out.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets rid of commas\n",
    "\n",
    "for filename in os.listdir(sourcefiledirectory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        #Read each text file into a string, and do the find-and-replace\n",
    "        f = open(filename, 'r', encoding='utf8')\n",
    "        text = f.read()\n",
    "        lines = [text.replace(\",\", \"\")]\n",
    "        \n",
    "        #Write output to a new file with the same name as the original, overwriting the original file.\n",
    "        with open(filename, 'w', encoding='utf8') as out:\n",
    "            out.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets rid of colons\n",
    "\n",
    "for filename in os.listdir(sourcefiledirectory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        #Read each text file into a string, and do the find-and-replace\n",
    "        f = open(filename, 'r', encoding='utf8')\n",
    "        text = f.read()\n",
    "        lines = [text.replace(\":\", \"\")]\n",
    "        \n",
    "        #Write output to a new file with the same name as the original, overwriting the original file.\n",
    "        with open(filename, 'w', encoding='utf8') as out:\n",
    "            out.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets rid of « quotation marks\n",
    "\n",
    "for filename in os.listdir(sourcefiledirectory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        #Read each text file into a string, and do the find-and-replace\n",
    "        f = open(filename, 'r', encoding='utf8')\n",
    "        text = f.read()\n",
    "        lines = [text.replace(\"«\", \"\")]\n",
    "        \n",
    "        #Write output to a new file with the same name as the original, overwriting the original file.\n",
    "        with open(filename, 'w', encoding='utf8') as out:\n",
    "            out.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets rid of » quotation marks\n",
    "\n",
    "for filename in os.listdir(sourcefiledirectory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        #Read each text file into a string, and do the find-and-replace\n",
    "        f = open(filename, 'r', encoding='utf8')\n",
    "        text = f.read()\n",
    "        lines = [text.replace(\"»\", \"\")]\n",
    "        \n",
    "        #Write output to a new file with the same name as the original, overwriting the original file.\n",
    "        with open(filename, 'w', encoding='utf8') as out:\n",
    "            out.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces ellipsis characters\n",
    "\n",
    "for filename in os.listdir(sourcefiledirectory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        \n",
    "        #Read each text file into a string, and do the find-and-replace\n",
    "        f = open(filename, 'r', encoding='utf8')\n",
    "        text = f.read()\n",
    "        lines = [text.replace(\"…\", \"\")]\n",
    "        \n",
    "        #Write output to a new file with the same name as the original, overwriting the original file.\n",
    "        with open(filename, 'w', encoding='utf8') as out:\n",
    "            out.writelines(lines)\n",
    "        out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Removing multiple spaces\n",
    "\n",
    "At this point, there are still places in the texts with five spaces (after previous chapter headers). This removes them and replaces them with a single space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(sourcefiledirectory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        f = open(filename, 'r', encoding='utf8')\n",
    "        text = f.read()\n",
    "        lines = [text.replace(\"     \", \" \")]\n",
    "\n",
    "        with open(filename, 'w', encoding='utf8') as out:\n",
    "            out.writelines(lines)\n",
    "        out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word vector creation\n",
    "\n",
    "The code blocks in this section generate the word vector representation for a set of texts. You can specify a different directory than the one used for the data cleaning, which can be useful if you want to run the vectors separately for different subsets of your corpus (e.g. just \"Harry Potter\", just \"Tanya Grotter\", etc.) To do this, copy the cleaned up text files for the subset of the corpus that you want to run into a new directory, and put the path to that new directory in the first code block below for *vector_sources*.\n",
    "\n",
    "Before you run this for the first time, you need to install the *gensim* Python package. Open a terminal window and type `pip install gensim`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0a One-time setup: install gensim\n",
    "The fist time you run this notebook, run the code cell below to install the `gensim` package. You won't have to run this the next time you run the notebook, but nothing bad will happen if you do run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#Installs gensim\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Run every time: import modules\n",
    "Run the code cell below every time to import the modules you need to run the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim is a Python module for generating and analyzing word vectors\n",
    "import gensim\n",
    "# Logging allows you to watch the progress of long-running processes\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# word2vec is used to generate the vectors, phrases to identify phrases as an input for vector generation\n",
    "from gensim.models import word2vec, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "# These utilities are used for exporting and loading models\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Specify where the text files are for the word vectors\n",
    "This may be the same place as you indicated above for the text cleaning steps, or you may choose to split your corpus into various subsets for creating word vectors.\n",
    "\n",
    "Change the value of *vector_sources* to where you've put your source files, then run the code block below.\n",
    "\n",
    "For instance, the default path a text file in the Documents directory is (substituting your user name on the computer for YOUR-USER-NAME):\n",
    "\n",
    "* On Mac: '/Users/YOUR-USER-NAME/Documents/YOUR-TEXT-FILE.txt'\n",
    "* On Windows: 'C:\\\\\\Users\\\\\\YOUR-USER-NAME\\\\\\Documents\\\\\\YOUR-TEXT-FILE.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_sources=\"/Users/qad/Documents/megacorpus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 List each file and its length\n",
    "This is a confirmation step that lists all the files that will be used as the input for the word vectors, along with how many characters are in each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change directory to where the data for your word vectors is\n",
    "os.chdir(vector_sources)\n",
    "#List all the documents in the directory with the data for your word vectors\n",
    "documents = list()\n",
    "for filename in glob.glob(\"*.txt\"):\n",
    "    #Open each text file in the directory and read it into a string\n",
    "    f = io.open(filename, mode=\"r\", encoding=\"utf-8\")\n",
    "    filedata = f.read()\n",
    "    #Print the filename along with how many characters (i.e. letters, numbers, etc.) are in the file\n",
    "    print(filename + \" = \" + str(len(filedata)) + \" chars\")\n",
    "    documents = documents + filedata.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Identify phrases\n",
    "This code block identifies bigram and trigram (2-word and 3-word, respectively) phrases. Especially if you have a small corpus, phrase mis-identification is possible through repeated words (e.g. \"she said\" in English).\n",
    "\n",
    "Phrases are treated like single words when doing the word vector generation.\n",
    "\n",
    "**Note:** this will take some time, and will generate a lot of status messages in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates bigrams and trigrams from the text\n",
    "sentence_stream = [doc.split(\" \") for doc in documents]\n",
    "trigram_sentences_project = []\n",
    "bigram = Phraser(Phrases(sentence_stream))\n",
    "trigram = Phraser(Phrases(bigram[sentence_stream]))\n",
    "\n",
    "for sent in sentence_stream:\n",
    "    bigrams_ = bigram[sent]\n",
    "    trigrams_ = trigram[bigram[sent]]\n",
    "    trigram_sentences_project.append(trigrams_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Running and saving word vectors\n",
    "This code block sets the parameters for vector generation, generates vectors, and saves the model.\n",
    "\n",
    "The default parameters should work in most cases. If you change the *num_features*, you'll need to change it again in the visualization code below.\n",
    "\n",
    "**Note:** this will take some time and generate a lot of status messages in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets values for various parameters for vector generation.\n",
    "num_features = 200    # Word vector dimensionality                      \n",
    "min_word_count = 2    # Minimum word count                        \n",
    "num_workers = 20      # Number of threads to run in parallel\n",
    "context = 5           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "# Sets up the code to run the word vector creation\n",
    "model = word2vec.Word2Vec(trigram_sentences_project, workers=num_workers, \\\n",
    "            vector_size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "\n",
    "# Saves model; you can change the name as long as it ends in .model\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the total number of items (including words, phrases, standalone punctuation, etc.) in the model's vocabulary\n",
    "\n",
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word vector analysis\n",
    "\n",
    "The code blocks below allow you to pull up most-similar and most-dissimilar terms, and attempt analogies with the word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a Using an existing model\n",
    "If you don't want to create your own word vectors, you can use one of [the pretrained models provided by Facebook](https://fasttext.cc/docs/en/pretrained-vectors.html). Download a model, put in the full path to it on your own computer below, and then run the code below.\n",
    "\n",
    "**You only need to do this if you haven't generated your own word vectors below. Otherwise, skip the following code block!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT run this if you've already created your own word vectors. This is ONLY for loading an existing model.\n",
    "from gensim.models.wrappers import FastText\n",
    "model = FastText.load_fasttext_format('/Users/qad/Downloads/wiki.vi/wiki.vi.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Most similar terms\n",
    "Put any word in the corpus between the quotes below to show the most similar words. You can change the value of *topn* to show more, or fewer, words.\n",
    "\n",
    "Keep in mind that if you used the preprocessing steps, the text is all lower-case and lemmatized, so no capital letters or inflected forms or else it will throw an error about the word not being in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = \"hội\"\n",
    "model.wv.most_similar (positive=w1,topn=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Most dissimilar terms\n",
    "Put any word in the corpus between the quotes below to show the most **dissimilar** words (i.e. those words that are used in the most dissimilar ways to the one you've given the model). You can change the value of *topn* to show more, or fewer, words.\n",
    "\n",
    "Keep in mind that if you used the preprocessing steps, the text is all lower-case and lemmatized, so no capital letters or inflected forms or else it will throw an error about the word not being in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = \"таня\"\n",
    "model.wv.most_similar (negative=w1,topn=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Analogies\n",
    "Without a very large corpus, the results of these analogies is very dissatisfying. The code below shows how to construct these analogies if you want to try them.\n",
    "\n",
    "The analogy code takes three words as input. To render the analogy гарри:квиддич::таня:??? (one might imagine драконбол as a high probability answer), you would use the code below. Or, more abstractly, given *A:B::C:??*, the code would be: `positive=['A','C'],negative=['B']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# гарри is to квиддич what таня is to...\n",
    "model.wv.most_similar(positive=['гарри','таня'],negative=['квиддич'],topn=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "The code below will generate two kinds of visualizations for the word vectors, by reducing the dimensionality of the vectors from 200 dimensions (or however many you specified when creating the vectors) down to 2.\n",
    "\n",
    "For this to work, you need the most recent version of *matplotlib*; as of March 2019, you may need to open a terminal and run `conda uninstall matplotlib` then `conda install matplotlib` to update to the latest version, depending on when you installed Anaconda.\n",
    "\n",
    "You also need to install the *sklearn* module. In the terminal: `pip install sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Visualizing similar words\n",
    "The code below will plot a given word, and the most similar words to it. You can increase or decrease the number of values displayed by changing the *topn* value for *close_words* (currently 30).\n",
    "\n",
    "You can input the word you want to use as the basis for similar words in the second code block below.\n",
    "\n",
    "Note: if you changed the number of features to something other than 200 when generating word vectors, you'll need to change the line `arr = np.empty((0,200), dtype='f')`, replacing 200 with the number of features you used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualizing subset of vectors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "from sklearn.manifold import TSNE\n",
    "def display_closestwords_tsnescatterplot(model, word):\n",
    "    \n",
    "    arr = np.empty((0,200), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.wv.most_similar (word, topn=30)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model.wv[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.scatter(x_coords, y_coords)\n",
    "    \n",
    "\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put your word between the single quotes here\n",
    "display_closestwords_tsnescatterplot(model,'снитч')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualizing all the words\n",
    "To visualize the overall shape of all the word vectors, you can run the code below. You'll need to zoom in quite a bit to be able to make anything specific out of it. Certain traits (like a loop in the overall curve) may be warning signs of data cleaning issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.io import push_notebook, show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "output_notebook()\n",
    " \n",
    "X = []\n",
    "for word in model.wv.vocab:\n",
    "    X.append(model.wv[word])\n",
    " \n",
    "X = np.array(X)\n",
    "print(\"Computed X: \", X.shape)\n",
    "X_embedded = TSNE(n_components=2, n_iter=250, verbose=2).fit_transform(X)\n",
    "print(\"Computed t-SNE\", X_embedded.shape)\n",
    " \n",
    "df = pd.DataFrame(columns=['x', 'y', 'word'])\n",
    "df['x'], df['y'], df['word'] = X_embedded[:,0], X_embedded[:,1], model.wv.vocab\n",
    " \n",
    "source = ColumnDataSource(ColumnDataSource.from_df(df))\n",
    "labels = LabelSet(x=\"x\", y=\"y\", text=\"word\", y_offset=8,\n",
    "                  text_font_size=\"20pt\", text_color=\"#555555\",\n",
    "                  source=source, text_align='center')\n",
    " \n",
    "plot = figure(plot_width=400, plot_height=400)\n",
    "plot.circle(\"x\", \"y\", size=12, source=source, line_color=\"black\", fill_alpha=0.8)\n",
    "plot.add_layout(labels)\n",
    "show(plot, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Acknowledgements\n",
    "\n",
    "Thanks to [Jeff Tharsen](http://tharsen.net/) for sharing a notebook that ran word vectors with gensim on Shakespeare. That notebook got this project off the ground by giving me an example of how to actually invoke the gensim phraser and word vector creation. The code in sections 3 and 4 is based off that notebook.\n",
    "\n",
    "Thanks to Aneesha Bakharia for [this Medium post on *Using TSNE to Plot a Subset of Similar Words from Word2Vec*](https://medium.com/@aneesha/using-tsne-to-plot-a-subset-of-similar-words-from-word2vec-bb8eeaea6229), where I found the code that section 5.1 is based on.\n",
    "\n",
    "Thanks to Jeff Thompson for [this blog post on visualizing word vectors](https://www.jeffreythompson.org/blog/2017/02/13/using-word2vec-and-tsne/), which I reworked slightly for section 5.2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# Digital Humanities Across Borders

_DLCL 204, CompLit 204A_

_Fall 2020_

_Asynchronous online_

Quinn Dombrowski (DLCL Academic Technology Specialist)

# Overview

English-language resources have dominated the discourse of digital humanities across the globe. This course takes a broader view, focusing on the methods, tools, and discourse of digital humanities as applied to textual materials in languages other than English. Students will develop practical skills in applying digital humanities research methodologies to texts in any language of their choosing. In addition, students will become familiar with major digital humanities scholarly organizations, movements, and debates that have their origins in different linguistic and cultural identities. No prior technical or digital humanities experience required, but students must have a reading knowledge of at least one non-English language (modern or historical).

# Credits / Grading

This course will be offered at Stanford for 3-5 credits, and for either a grade or credit/no credit. Students who wish to take it as part of the DH Minor must choose 5 credits (if taken as a core course) or 3 credits (if taken as an elective) and must take it for a letter grade.

The course will use contract grading, where students choose what grade they wish to receive, and write a contract (within defined parameters) at the beginning of the quarter that lays out the requirements for receiving that grade. Individual assignments will receive extensive feedback but will be graded as accepted / needs revisions. Students will have one week to revise assignments that need revisions to fulfill the terms of their contract. If a student is unable to fulfill the terms of their original contract, they will meet with the instructor and sign a new contract for a different grade.

# Assignments

This course will involve doing less reading than most courses in the humanities, and there will be neither papers nor exams. Instead, you&#39;ll get experience with reading and producing different kinds of scholarly production through 3-4 "major assignments" that will take the form of some alternative mode of scholarly production (e.g. conference proposals, posters, tutorials, blog posts, podcasts, videos, games).

In lieu of synchronous in-class discussion, there'll be weekly readings to comment on and discuss with your classmates using the Hypothes.is annotation tool (which may be available through Canvas, or may require a separate browser plug-in.) This syllabus will be updated with those readings after the course starts, as the selection of texts will be shaped by students' languages and interests. Occasionally, there'll be video lectures to watch, with opportunities to ask questions and discuss using the course message board. I'll also try to arrange and record an interview with someone doing DH work in each of the languages students are working with, and you'll have an opportunity to contribute questions to those interviews.

## Major assignments

Your path through this course will be shaped by the language and text you're working with, and your research questions / area of inquiry. Almost all text-based DH projects share the same broad trajectory:

* **Text acquisition**: Getting the text in a form that you can use for the subsequent steps. Can include scanning, OCR (turning pictures of text into editable text), handwritten text recognition, web scraping, etc.
* **Enrichment**: Doing something to your text to make it possible to do analysis / visualization. May include various forms of cleaning the data, annotating it (manually, or using algorithms to extract person and place names), adding metadata, etc.
* **Analysis/visualization**: Analyzing or visualizing the text in some way using the enrichment you've done. Includes things like topic modeling, text comparison, network analysis, mapping, etc.
* **Interpretation**: What sense can you actually make of the analysis, given what you know already about the subject matter? 

The course will be broken up into segments based on these different phases. Students will complete one major assignment for text acquisition, enrichment, and analysis/visualization (3 credits). 5-credit students will also complete a major assignment for interpretation. 

Major assignments can take many different formats, for instance:

* A set of texts that you've scanned and OCR'd, with a 1-page written reflection on the process (what worked well, what was challenging, what surprised you, etc.)
* A collaborative podcast with some of the other students working on the same language or tool, comparing your experiences working on a particular phase of the project
* A step-by-step tutorial for how to use one of the tools for your project, including details specific to your language
* A pair of conference proposals for your project, one for a conference in your discipline, and another for a DH conference
* A choose-your-own-adventure game/story that takes the reader through major decision points when working on a textual DH project in your language

I'll prepare some examples before the class starts, but other creative formats are also welcome! Check with me if you have any questions.

By default, each major assignment will be due after the corresponding phase of the class, to help you stay on track throughout the quarter. That said, the deadlines can be flexible if you need more time on one or more of the assignments; please let me know if you'd like an extension. All work should be turned in by November 30th at the latest, so I can get you feedback and you can do any revisions prior to the *absolutely final deadline* of December 4th (grades are due on the 8th).


# Weekly schedule

## Week 1 (September 14): Introductions
One of the challenges of teaching this course is that I don't know what languages people are interested in until they arrive. (If you're enrolling in the class, it helps if you email me in advance and let me know what language you'll be working with!) The first week will give us a chance to get to know one another, and I'll try to find some existing DH work that might resonate with your linguistic and topical interests. I'll also share a few different perspectives on what DH is, and what it looks like in practice to do DH work.

During week 1, I'll start holding 1:1 meetings with students to talk about what they're interested in working on this quarter.

This week's readings are an introduction to major themes you'll encounter in the communities that make up digital humanities:

* Lisa Spiro's "['This is Why We Fight: Defining the Values of Digital Humanities](https://dhdebates.gc.cuny.edu/read/untitled-88c11800-9446-469b-a3be-3fdb36bfbd1e/section/9e014167-c688-43ab-8b12-0f6746095335)" (2012) looks to define shared values as a way to find common ground within the (still ongoing) debates about what DH is.
* The piece "[Donner à lire les humanités numériques francophones](https://journals.openedition.org/revuehn/508)" (2020) from the editorial board of *Humanités Numériques* (the journal of the Francophone DH association [Humanistica](http://www.humanisti.ca/)) talks about how the journal came to be, as part of the process of forming and defining the field of Francophone DH. If you don't comfortably read French, Google Translate works fine, but be sure to check back with the original particularly around terminology, which gets a little lost in the translation (e.g. in paragraph 5, '*"Digital Humanities" materialized in English-speaking countries - then everywhere else, sometimes with local names. The genesis of the journal is, in fact, deeply rooted in the European and French development of digital humanities.'* loses the fact that the first reference to "digital humanities" uses the English term, and the second uses the French term.)
* "[Digital Humanities: Is it Research or is it Service?](https://dhmuc.hypotheses.org/2834)" by Arnold Eckhart (2020) on the Digital Humanities München blog gets more concrete about what kinds of tasks go into doing DH, and reflects on the extent to which they should be considered "research" or "service". (Over the course of the quarter, you'll see other perspectives that challenge how meaningful or useful this dichotomy is, but it covers some common ways of talking and thinking about DH.)
* "[Leaving Humanist](https://linguacelta.com/blog/2020/08/Humanist.html)", a blog post by UK scholar Bethan Tovey-Walsh, is a piece for the present moment. This summer, a number of major DH organizations issued statements on Black Lives Matter, structural racism, and establishment violence. (We'll read these later in the quarter.) Shortly thereafter, Willard McCarty, who won an award for lifetime achievement in DH in 2013, approved a post referencing "anti-white racism" on [Humanist](https://dhhumanist.org/), the DH discussion mailing list he has curated since the late 1980's. His reluctance to approve responses to this post caused a backlash from many DH scholars, and the list ultimately went on hiatus. This piece serves as a the counterweight to Spiro's values of digital humanities: what happens when people understand those values differently, particularly as they intersect with rapidly-evolving developments in some (but not all) national contexts?


## Week 2 (September 21): Text acquisition
During week 2, I'll continue holding 1:1 meetings with students to plan out the quarter. As those plans take shape, I'll start organizing meet-ups with groups of students, based on shared languages and/or tools. These will take place throughout the quarter.

Students will start to work on the text acquisition phase of their project.

* [‘Q i-jtb the Raven’: Taking Dirty OCR Seriously](https://ryancordell.org/research/qijtb-the-raven/), Ryan Cordell, 2017. This piece dives into the metadata you leave behind in the EXIF file structure, as someone who digitizes a text that was originally printed. How are digitized works more than just a machine-readable version of the original? What's the "materiality" of a digitized text? As you read this, think about your own digitized texts. What path do you think they took to get that way? If you're scanning and OCRing books, think about how your choices impact the final version. The references to "CA" are about "Chronicling America" an effort to digitize American newspapers that excluded Japanese-language newspapers, among others. (Locally, the [Hoover Institute](https://www.hoover.org/library-archives/collections/japan) has a project to fill in that gap.)
* [Recognizing handwritten text in Slavic manuscripts: A neural-network approach using Transkribus](https://www.academia.edu/38835297/Recognizing_handwritten_text_in_Slavic_manuscripts_A_neural_network_approach_using_Transkribus_1_Achim_Rabus) by Achim Rabus. This piece is, fundamentally, a write-up of one scholar's experiences with using the Transkribus software to create a model for digitizing Slavic manuscripts. The Russian folks in the class have a leg up here, but honestly, even if you can't recognize any of the Cyrillic alphabet, reading this is a very real-world experience. (I don't know the first thing about Ottoman Turkish, but I'm reading some comp sci papers about machine learning methods on how to translate it, this week.) Don't get bogged down into the techical (or Slavistic) details if that's not your thing. What are the use cases where Achim describes trying it out? Does Transkribus work well for all those use cases? If you were teaching a DH class and you had a student interested in Slavic Cyrillic manuscripts, would you recommend it? Why or why not?
* Watch [this news report on Tarin Clanuwat and her machine-learning model for kuzushiji](https://www.facebook.com/cnbctv18india/posts/2540954169275299). This is for a general audience; you can also read more detail on the [Around DH 2020 piece](http://arounddh.org/en/kuronet) about the project. (Use the Around DH 2020 piece for annotating.)
* "The DSC and Mean Copyright Law", coming Tuesday from the Data-Sitters Club. Especially if you're working with e-books, this is for you.


## Week 3 (September 28): Text acquisition / preparing for enrichment
Students will continue to work on text acquisition during week 3, and will prepare their assignment for the text acquisition phase. I'll be available throughout the week for troubleshooting help and advice.

* With a week and a half until the first major assignment is due, you should be working on preparing your folder of plain-text files and your spreadsheet with information about each of the text files. Some of you will only have a couple texts, so it won't be hard to make sure that the spreadsheet is consistent. Some of you are going to have more than 20 texts, and the more texts you have, the easier it is to make mistakes. Read through [this tutorial on OpenRefine](https://programminghistorian.org/en/lessons/cleaning-data-with-openrefine) by Seth van Hooland, Ruben Verborgh, and Max De Wilde, and try it out on your spreadsheet if you think it'd be helpful. (You don't need to annotate this one, but feel free to ask me any questions.)
* Following up on last week's lecture on Unicode, we have a critique of Unicode from the perspective of its imposition of a western, alphabetic model of semiotics on scripts whose own organizing principles take a different form (using Devanagari and Arabic script as examples). Paolo Monella was kind enough to share a preprint of "Scritture dimenticate, scritture colonizzate: sistemi grafici e codifiche digitali nelle culture araba e indiana" with us, and I've put in the Canvas module both the preprint PDF itself (which renders all the text correctly), and a version where I exported the text from the PDF, much like many of you are doing for your corpora. In a moment of irony that really drives home some of the points this piece is making, exporting the text from Acrobat resulted in all the non-Latin characters being dropped altogether, and many of the Latin characters with diacritics were also dropped. So the references to "fu" are in fact *fuṣḥа̄*. The piece is in Italian, and at least when I ran the whole page through Google Translate in the browser, some arbitrary paragraphs didn't translate. But if you put those into Google Translate individually, it works fine. (FWIW, I don't read Italian, so I'm not asking you to do anything in making your way through this text that I didn't do, too.)
* In anticipation of next week when we'll start working on enrichment, "[Preparing Non-English Texts for Computational Analysis](https://www.modernlanguagesopen.org/articles/10.3828/mlo.v0i0.294/)" (which I wrote last year) covers some of the major things that you need to do to non-English text before using it with any kind of analysis involving counting words.



## Week 4 (October 5): Enrichment
This week you should be close to having your first assignment (your corpus) ready. (If not, let me know and we'll figure something out!) Now that you have some text, it's time to start doing things with it. You'll soon be splitting off again to work on your respective projects, but for one week, let's start exploring your many (and very different) corpora using the same tool: [Voyant](https://voyant-tools.org/). This will also be the week when you'll get to try out Jupyter notebooks for pre-processing your text -- and you can compare how raw vs. pre-processed non-English text behaves in Voyant. Here's the combo reading list / to-do list for this week:

* **Read/Annotate**: Ch. 1 ("Introduction: Correcting Method") and Ch. 2 ("The Measured Words: How Computers Analyze Texts") of *Hermeneutica: Computer-Assisted Interpretation in the Humanities* by Stéfan Sinclair & Geoffrey Rockwell. These chapters form an introduction to thinking about what humanities-oriented computational text analysis is, and what it does (and what it can't do). Hermeneutica is [available online via Searchworks](https://searchworks.stanford.edu/view/13347297), and I'll have Hypothesis-enabled versions on Canvas.
* **Do**: Upload the text files from your corpus to [Voyant](https://voyant-tools.org/) and explore some of the different tools available. What do you notice? How well does it work?
* **Read/Do**: [Introduction to Jupyter Notebooks](https://programminghistorian.org/en/lessons/jupyter-notebooks): this Programming Historian tutorial (which I co-authored... sorry... criticism is very welcome!) explains what Jupyter notebooks are, both conceptually and practically. Follow the instructions in the tutorial to install Anaconda on your computer. Many of you will be using Jupyter notebooks for text pre-processing.
* **Do**: For languages other than Chinese, Japanese, and Vietnamese, download the lemmatizer notebook for your language, and run it on your text to create lemmatized derivative files (where all words are replaced with their dictionary forms). Upload **those** files to Voyant. How are your results different? (Chinese, Japanese, and Vietnamese-language students can skip this step.)
* **Do**: Post a short paragraph to Slack or Canvas about your experiences with Voyant, and lemmatized vs. non-lemmatized text.
* **Do**: Fill out the Doodle poll for trying to find a time for your language group to meet synchronously (will be posted to your language's Slack channel on Monday).
* **Optional**: For a colloquial walkthrough of how you might follow a research question through multipel Voyant tools, here's [Data-Sitters Club #6: Voyant's Big Day](https://datasittersclub.github.io/site/dsc6/).




## October 9: Major assignment 1 due
The text acquisition major assignment is due, unless you've arranged for a different due date.

## Week 5 (October 12): Word vectors
Word vectors aren't the place that you'd usually start with exploring your text -- or even the place you'd go next after Voyant. But more than a couple of you are interested in playing with word vectors as part of your final project, and in most cases you want to use the lemmatized version of your corpus for word vectors -- and last week we covered how to lemmatize your corpus. Next week we'll get into text comparison, which will require you to do part-of-speech tagging on your corpus, but for this week, you can use the lemmatized versions of your texts for word vectors.

* **Read/Annotate**: Ryan Heuser's website is back up! I've sent these links to some of you before, but read and annotate his 4-part series about word vectors: [part 1](https://ryanheuser.org/word-vectors-1/), [part 2](https://ryanheuser.org/word-vectors-2/), [part 3](https://ryanheuser.org/word-vectors-3/), [part 4](https://ryanheuser.org/word-vectors-4/)
* **Read** The last time I taught this course, I explored word vectors for the first time for my own final project. It's not necessarily the best example of how you'd got about using it, but it's what I was able to come up with, as a way to try to [compare the Russian translation of Harry Potter and a work that was found to be copyright infringement vis-a-vis Harry Potter](https://github.com/quinnanya/dlcl204/blob/master/harry_potter_tanya_grotter_project_2019/dombrowski_dlcl204poster_2019.pdf).
* **Try** generating word vectors based on your corpus using this Jupyter notebook. If your corpus has less than 1 million words, you might want to try playing with one of the [pre-trained models for your language](https://fasttext.cc/docs/en/pretrained-vectors.html) (e.g. these models are provided by Facebook).
* For the Classics students, here's a paper on "[Word Embeddings Pointing the Way for Late Antiquity](https://www.aclweb.org/anthology/W15-3708.pdf)" by Johannes Bjerva and Raf Praet (no need to annotate, just FYI)


## Week 6 (October 19): Catch-up & text comparison
This week is a good opportunity to catch up on any material you haven't had a chance to get through, and start working on a plan for your final projet, which should help shape decisions about what you want to do for assignments 2 and 3.

The only reading for this week is a long one: an all-new Data-Sitter's Club book on text comparison that goes in-depth with a few different text comparison methods. It also doesn't shy away from the places where you can (and I **did**) mess things up due to simple mistakes and misunderstandings with code. 

If you'd like to try out some of the methods described in the book, there's also a "just the code" notebook that you can play with. (You probably will want to use your lemmatized text, if that's relevant for your language.) If you want to explore further refining your corpus before you compare your texts (e.g. just pulling out all nouns), let me know and we can put together a Jupyter notebook to do it!

* Read/annotate: *[Data-Sitters Club #8: Text-Comparison-Algorithm-Crazy Quinn](https://datasittersclub.github.io/site/dsc8/)*

## October 23: Major assignment 2 due
The enrichment major assignment is due, unless you've arranged for a different due date.

## Week 7 (October 26): Named entity recognition
A number of students are interested in the people and places that occur in their corpus. Named entity recognition is one way to try to identify those, particularly when you don't have a pre-defined list of names or places that you're interested in. (If you do, you're better off just searching for the things on your list, in a manner sensitive to possible variations.)

* Read/annotate: *[Data-Sitters Club Multilingual Mystery #2: Beware Lee and Quinn!](https://datasittersclub.github.io/site/dscm2/): this DSC book compares how well the Spacy named entity recognition for English works, compared to French. (You can guess which one works better.) There's a link to a Jupyter notebook with code for running Spacy NER for English and French; if your language's lemmatizer code used Spacy, you can try adapting the code in the DSC notebook for your language. (All you should have to do is change which model it loads; compare the first part of the notebook to your language's lemmatizer notebook.) If your language didn't use Spacy, talk with Quinn about what other tools you can use. (If you'd like, Quinn can put together a notebook that you can try out for your language.)
* Read/annotate: Katherine McDonough, Ludovic Moncla & Matje van de Camp. "[Named entity recognition goes to old regime France: geographic text analysis for early modern French corpora](https://www.tandfonline.com/doi/abs/10.1080/13658816.2019.1620235)". 


## Week 8 (November 2) & Week 9 (November 9): Project work / topic modeling
Over these two weeks, students will mostly be working on their own projects, which will lead to major assignment 3 (and 4, for 5-credit students). Based on students' project needs, I'll be putting together tutorials around specific topics, including potentially training models, network visualizations, and general data visualization with Tableau. The last method that we'll cover together is topic modeling.

* Read/annotate: *[The Joy of Topic Modeling](http://mcburton.net/blog/joy-of-tm/)* by Matt Burton
* Read/annotate: *[The LDA Buffet is Now Open; or, Latent Dirichlet Allocation for English Majors](https://www.matthewjockers.net/2011/09/29/the-lda-buffet-is-now-open-or-latent-dirichlet-allocation-for-english-majors/)* by Matt Jockers
* Play with [jsLDA: In-browser topic modeling](https://mimno.infosci.cornell.edu/jsLDA/) (remember, if you're using your own data, you need to upload multiple "documents", AKA "some piece of text"). You may also enjoy [this tweet stream](https://twitter.com/dmimno/status/1320736660150198273?s=19) by David Mimno explaining the design goals.

## Week 10 (November 16): Wrap-up
As a class (collectively, and in language- and/or tool-specific sub-groups) we'll consider what we've learned through this class, and how we can apply it moving forward in different contexts (in other classes, as CV/resumé fodder, as a way to pursue questions that interest you). 

On the last day of class (Friday, November 20th), we'll have a synchronous Zoom "poster session" where everyone can present their work from the class (optional but encouraged).

## November 20: Major assignment 3 due
The analysis major assignment is due, unless you've arranged for a different due date.

## November 23: All work due
November 23rd is the deadline for turning in all work (including major assignment 4 for 5-credit students), with enough time for me to send you feedback and you to do revisions, if needed.


# Format and COVID-19
This course has been completely restructured into an online, asynchronous course in response to COVID-19, with all the potential associated challenges around time zones, connectivity, and combining education with increased care responsibilities. This format does have advantages: one of the biggest challenges for people working on non-English DH while based in the United States is that it can be challenging to build community locally, or even regionally. Running DLCL 204 asynchronously online makes it feasible for more people to participate, regardless of their location.

## Zoom
1:1 student meetings, open office hours, student meet-ups, and some troubleshooting sessions will take place via Zoom. Other than the 1:1 student meetings, participation in these meetings is encouraged but optional. Use of video on Zoom is not required, but particularly for troubleshooting and student meet-ups, you may find it helpful to share your screen.

## Slack
Everyone will be working on their projects at a different pace, at different times of day. To avoid the turnaround time of exchanging email every time you have a question, I'll set up a Slack channel for the class, or you can message me privately. Use of Slack is not required, but may make it faster to get the answers you're looking for -- from me, or your classmates.

# Accommodation

I want this class to be accessible to anyone who shows up with an interest in the topic. Everyone has a right to the full experience of the class — which is fundamentally about what role digital tools and methods might play in your education and/or research, and how you might choose to connect with and contribute to communities of "digital" scholarship. Grappling with your identity as a student and/or scholar can be difficult. If there's something in the syllabus or that we cover in class that freaks you out, please talk to me! If you're encountering a barrier, or expecting a possible barrier to your being able to successfully complete your contract, let's chat! It doesn't need to be academic — sleep-training kids, taking care of unwell family and friends, roommates who listen to Viking metal bands at full volume at all hours of the night, and anxiously binge-watching Netflix are real life. Mental illness flare-ups can impact what you can get done — I know, I had a bipolar II diagnosis for all of grad school. Whether or not you've got paperwork through the Office of Accessible Education (<https://oae.stanford.edu/>), please come talk to me anytime if something is getting in the way of your learning or completing the contract you created.

The pandemic and all ongoing uncertainty around it are complicating life for everyone, in different ways. Regardless of your situation, I want this class to be an enriching, thought-provoking experience for you. Please get in touch (using any contact methods listed on Canvas that's comfortable for you) if anything comes up that you think might pose a challenge for you to successfully complete the class, and together we can figure out a way to work things out.

# Fundamental standard & honor code

Beyond the bare minimums laid out by the Stanford Fundamental Standard (<https://communitystandards.stanford.edu/policies-and-guidance/fundamental-standard>), I expect you to treat one another in this class not only with respect, but with generosity. If you find a resource or an approach that has helped you, share it so others can benefit -- and listen when others share.

The honor code (<https://communitystandards.stanford.edu/policies-and-guidance/honor-code>) lays out guidelines for the university's policy on academic integrity. Collaboration -- with your classmates and others -- is very much welcome in this course, but be sure to acknowledge your collaborators and the assistance they provided (e.g. including in your acknowledgements section a cousin who helped you interpret some of the statistics from your analysis.)


# Acknowledgements

Thanks to Miriam Posner (DH 150: Selfies, Snapchat, & Cyberbullies, <http://miriamposner.com/dh150w15/contract-grading/>) and Ryan Cordell (ENGL 1450: Reading and Writing in the Digital Age, <https://f18rwda.ryancordell.org/course-policies.html>) for the inspiration for contract grading.

Thanks to Aimée Morrison for inspiring the original accommodation statement, and Anastasia Salter for inspiring the COVID section.

Thanks to [Elisa Beshero-Bondar](https://newtfire.org/courses/introDH/introDHSyll.html) for the idea of including "Leaving Humanist" in the course readings.